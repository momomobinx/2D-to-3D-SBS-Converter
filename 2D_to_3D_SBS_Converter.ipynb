{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# 2D to 3D Side-by-Side Video Converter (GPU Optimized)\n",
        "\n",
        "This notebook converts standard 2D videos into stereoscopic 3D Side-by-Side (SBS) format for VR viewing. It uses MiDaS for depth estimation and creates a stereoscopic effect by synthesizing left and right eye views.\n",
        "\n",
        "## Features\n",
        "- Optimized for maximum GPU utilization (works with any available GPU memory)\n",
        "- Video segment selection for processing specific portions of longer videos\n",
        "- Upload videos (up to 500MB) or provide video URLs\n",
        "- Adjustable depth parameters (intensity, convergence, eye separation)\n",
        "- High-quality H.264 encoded MP4 output in SBS format with 16:9 overall aspect ratio\n",
        "- **Preserves original audio track** in the output video\n",
        "- Real-time preview and parameter adjustment\n",
        "- Progress tracking and error handling\n",
        "\n",
        "## Setup\n",
        "Run the installation cell below to set up the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "installation"
      },
      "source": [
        "# Install required packages\n",
        "!pip install opencv-python-headless\n",
        "!pip install numpy\n",
        "!pip install gradio\n",
        "!pip install torch torchvision\n",
        "!pip install timm\n",
        "!pip install yt-dlp\n",
        "!pip install pytube\n",
        "!apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "# Clone MiDaS repository and install its dependencies\n",
        "!git clone https://github.com/isl-org/MiDaS.git\n",
        "!pip install -q -r MiDaS/requirements.txt"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import all necessary libraries for video processing, depth estimation, and the user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import urllib.request\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import time\n",
        "import re\n",
        "import shutil\n",
        "import threading\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {gpu_mem:.2f} GB\")\n",
        "    # Set CUDA device to GPU 0\n",
        "    torch.cuda.set_device(0)\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "# Set default tensor type to cuda if available\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "# Optional: Set environment variable for PyTorch memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "midas_setup"
      },
      "source": [
        "## MiDaS Setup\n",
        "\n",
        "Initialize the MiDaS depth estimation model with GPU optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_midas"
      },
      "source": [
        "# Audio processing functions\n",
        "def check_audio_stream(file_path):\n",
        "    \"\"\"Check if the video file has an audio stream\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-select_streams', 'a:0',\n",
        "             '-show_entries', 'stream=codec_name', '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "             file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # If there's output, an audio stream was found\n",
        "        return bool(result.stdout.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking audio stream: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def extract_audio(input_path, output_path):\n",
        "    \"\"\"Extract audio from a video file using ffmpeg\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        # Use ffmpeg to extract audio\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', input_path,        # Input file\n",
        "            '-vn',                   # Disable video\n",
        "            '-acodec', 'copy',       # Copy audio codec without re-encoding\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        print(f\"Extracting audio from {input_path} to {output_path}...\")\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error extracting audio: {result.stderr}\")\n",
        "            return None\n",
        "\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            print(\"Audio extraction failed - output file is empty or missing\")\n",
        "            return None\n",
        "\n",
        "        print(\"Audio extracted successfully\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error during audio extraction: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def combine_video_audio(video_path, audio_path, output_path):\n",
        "    \"\"\"Combine video and audio files using ffmpeg\"\"\"\n",
        "    try:\n",
        "        # Use ffmpeg to merge video and audio\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', video_path,        # Video file\n",
        "            '-i', audio_path,        # Audio file\n",
        "            '-c:v', 'copy',          # Copy video without re-encoding\n",
        "            '-c:a', 'aac',           # Use AAC for audio (better compatibility)\n",
        "            '-b:a', '192k',          # Audio bitrate\n",
        "            '-shortest',             # Match the duration of the shorter file\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        print(f\"Combining video and audio into {output_path}...\")\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error combining video and audio: {result.stderr}\")\n",
        "            return False\n",
        "\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            print(\"Combination failed - output file is empty or missing\")\n",
        "            return False\n",
        "\n",
        "        print(\"Video and audio combined successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error during combination: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def setup_midas():\n",
        "    \"\"\"Initialize and return the MiDaS model for depth estimation using torch.hub\n",
        "    with optimizations for GPU usage\"\"\"\n",
        "    print(\"Loading MiDaS depth estimation model...\")\n",
        "\n",
        "    # Clean up any existing GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Select device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Print GPU info\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "    # Load model - using DPT Large for best quality\n",
        "    try:\n",
        "        # Try to disable torch hub cache to ensure we get a fresh model\n",
        "        torch.hub.set_dir(tempfile.mkdtemp())\n",
        "        midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Fallback method\n",
        "        print(\"Trying alternate loading method...\")\n",
        "        midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\", trust_repo=True)\n",
        "\n",
        "    midas.to(device)\n",
        "    midas.eval()  # Set to evaluation mode\n",
        "\n",
        "    # If using CUDA, optimize model for inference\n",
        "    if device.type == 'cuda':\n",
        "        # Enable cuDNN benchmark mode for best performance with fixed input sizes\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        # We'll skip TorchScript optimization as it's causing issues\n",
        "        print(\"Skipping TorchScript optimization due to compatibility issues\")\n",
        "\n",
        "    # Load transforms\n",
        "    try:\n",
        "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transforms: {e}\")\n",
        "        # Fallback\n",
        "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
        "\n",
        "    transform = midas_transforms.dpt_transform\n",
        "\n",
        "    # Report GPU memory usage after model loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory After Model Load: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "\n",
        "    print(\"MiDaS model loaded successfully!\")\n",
        "    return midas, transform, device"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_processing"
      },
      "source": [
        "## Video Processing Functions\n",
        "\n",
        "Functions for video input validation, frame extraction, audio processing, and depth map generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "video_functions"
      },
      "source": [
        "def validate_video(file_path):\n",
        "    \"\"\"Validate if the input video file is supported\"\"\"\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        return False, \"File does not exist\"\n",
        "\n",
        "    # Check file extension\n",
        "    valid_extensions = [\".mp4\", \".avi\", \".mov\", \".webm\", \".mkv\"]\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "    if file_ext not in valid_extensions:\n",
        "        return False, f\"Unsupported file format: {file_ext}. Supported formats: {', '.join(valid_extensions)}\"\n",
        "\n",
        "    # Check if OpenCV can open the file\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    if not cap.isOpened():\n",
        "        return False, \"Cannot open video file with OpenCV\"\n",
        "\n",
        "    # Check resolution\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    if width > 3840 or height > 2160:\n",
        "        cap.release()\n",
        "        return False, f\"Video resolution ({width}x{height}) exceeds maximum supported resolution (3840x2160)\"\n",
        "\n",
        "    # Check file size (500MB limit)\n",
        "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    if file_size_mb > 500:\n",
        "        cap.release()\n",
        "        return False, f\"File size ({file_size_mb:.2f}MB) exceeds maximum supported size (500MB)\"\n",
        "\n",
        "    # Get video info\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration_sec = frame_count / fps if fps > 0 else 0\n",
        "    cap.release()\n",
        "\n",
        "    return True, {\"width\": width, \"height\": height, \"fps\": fps, \"frame_count\": frame_count,\n",
        "                  \"size_mb\": file_size_mb, \"duration_sec\": duration_sec}\n",
        "\n",
        "def extract_video_segment(input_path, output_path, start_time, end_time):\n",
        "    \"\"\"Extract a segment from a video file using ffmpeg\"\"\"\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting segment from {start_time:.2f}s to {end_time:.2f}s...\")\n",
        "\n",
        "        # Use ffmpeg to extract the segment with stream copy\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-i', input_path,        # Input file\n",
        "            '-ss', str(start_time),  # Start time in seconds\n",
        "            '-to', str(end_time),    # End time in seconds\n",
        "            '-c:v', 'copy',          # Copy video stream without re-encoding\n",
        "            '-c:a', 'copy',          # Copy audio stream without re-encoding\n",
        "            '-avoid_negative_ts', '1',  # Avoid negative timestamps\n",
        "            '-y',                    # Overwrite output file if it exists\n",
        "            output_path\n",
        "        ]\n",
        "\n",
        "        # Run the command and capture output\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "        # Check if the command was successful\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error extracting segment: {result.stderr}\")\n",
        "            raise Exception(f\"ffmpeg error: {result.stderr}\")\n",
        "\n",
        "        # Verify the output file exists and has content\n",
        "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
        "            raise ValueError(\"Segment extraction failed - output file is empty or missing\")\n",
        "\n",
        "        print(f\"Segment extracted successfully: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting video segment: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def ensure_h264_mp4(input_path, temp_dir=\"temp_videos\"):\n",
        "    \"\"\"Convert video to H.264 MP4 format if needed - optimized for speed\"\"\"\n",
        "    # Generate a new filename for the converted video\n",
        "    output_path = os.path.join(temp_dir, f\"h264_{int(time.time())}.mp4\")\n",
        "\n",
        "    # Use ffprobe to check if the video is already H.264 encoded\n",
        "    try:\n",
        "        print(f\"Checking encoding of {input_path}...\")\n",
        "        # Get video codec information with a short timeout\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
        "             '-show_entries', 'stream=codec_name', '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "             input_path],\n",
        "            capture_output=True, text=True, check=True, timeout=10\n",
        "        )\n",
        "        codec = result.stdout.strip()\n",
        "\n",
        "        if codec.lower() in ['h264', 'avc1']:\n",
        "            print(f\"Video is already H.264 encoded (codec: {codec})\")\n",
        "            return input_path\n",
        "        else:\n",
        "            print(f\"Video is not H.264 encoded (detected codec: {codec}). Converting with fast settings...\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"Codec detection timed out. Proceeding with conversion...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking video codec: {str(e)}. Converting with fast settings...\")\n",
        "\n",
        "    # Check if input file exists and has content\n",
        "    if not os.path.exists(input_path) or os.path.getsize(input_path) == 0:\n",
        "        raise ValueError(f\"Input file {input_path} does not exist or is empty\")\n",
        "\n",
        "    # Convert to H.264 MP4 with hardware acceleration if available\n",
        "    try:\n",
        "        print(\"Starting fast H.264 conversion...\")\n",
        "\n",
        "        # Try using hardware acceleration if available\n",
        "        # NVIDIA GPU acceleration\n",
        "        hw_accel_commands = [\n",
        "            # NVIDIA NVENC (if available)\n",
        "            [\n",
        "                'ffmpeg',\n",
        "                '-i', input_path,\n",
        "                '-c:v', 'h264_nvenc',  # NVIDIA GPU acceleration\n",
        "                '-preset', 'p1',  # Fast encoding preset\n",
        "                '-tune', 'hq',  # High quality tuning\n",
        "                '-rc:v', 'vbr',  # Variable bitrate\n",
        "                '-cq:v', '23',  # Quality level\n",
        "                '-b:v', '5M',  # Target bitrate\n",
        "                '-maxrate:v', '10M',  # Maximum bitrate\n",
        "                '-bufsize:v', '10M',  # Buffer size\n",
        "                '-c:a', 'aac',  # Audio codec\n",
        "                '-b:a', '128k',  # Audio bitrate\n",
        "                '-y',  # Overwrite output if exists\n",
        "                output_path\n",
        "            ],\n",
        "            # Fallback to CPU with ultrafast preset\n",
        "            [\n",
        "                'ffmpeg',\n",
        "                '-i', input_path,\n",
        "                '-c:v', 'libx264',  # CPU encoding\n",
        "                '-preset', 'ultrafast',  # Fastest encoding\n",
        "                '-tune', 'fastdecode',  # Fast decoding optimization\n",
        "                '-crf', '28',  # Lower quality for speed\n",
        "                '-g', '30',  # Keyframe every 30 frames\n",
        "                '-bf', '0',  # No B-frames (faster)\n",
        "                '-c:a', 'aac',  # Audio codec\n",
        "                '-b:a', '128k',  # Low audio bitrate\n",
        "                '-ac', '2',  # Stereo audio\n",
        "                '-ar', '44100',  # Standard audio sample rate\n",
        "                '-strict', 'experimental',\n",
        "                '-y',  # Overwrite output\n",
        "                output_path\n",
        "            ]\n",
        "        ]\n",
        "\n",
        "        # Try each acceleration method in order\n",
        "        success = False\n",
        "        for i, command in enumerate(hw_accel_commands):\n",
        "            try:\n",
        "                print(f\"Trying encoding method {i+1}...\")\n",
        "\n",
        "                # Run the command\n",
        "                print(f\"Running conversion command: {' '.join(command)}\")\n",
        "                process = subprocess.Popen(\n",
        "                    command,\n",
        "                    stdout=subprocess.PIPE,\n",
        "                    stderr=subprocess.PIPE,\n",
        "                    universal_newlines=True\n",
        "                )\n",
        "\n",
        "                # Set timeout for conversion (5 minutes)\n",
        "                timeout = 300  # seconds\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Monitor progress\n",
        "                while process.poll() is None:\n",
        "                    # Check if timeout has been reached\n",
        "                    if time.time() - start_time > timeout:\n",
        "                        process.terminate()\n",
        "                        raise TimeoutError(f\"Conversion timed out after {timeout} seconds\")\n",
        "\n",
        "                    # Print progress indicator\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "                    time.sleep(1)\n",
        "\n",
        "                # Check if successful\n",
        "                if process.returncode == 0 and os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                    print(f\"\\nSuccessfully converted to H.264 MP4 using method {i+1}: {output_path}\")\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"\\nMethod {i+1} failed with error code {process.returncode}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with method {i+1}: {str(e)}\")\n",
        "\n",
        "        if success:\n",
        "            return output_path\n",
        "        else:\n",
        "            # Fallback to simple copy method (no re-encoding)\n",
        "            try:\n",
        "                print(\"Attempting direct copy method as fallback...\")\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', input_path,\n",
        "                    '-c', 'copy',  # Just copy streams without re-encoding\n",
        "                    '-y',\n",
        "                    output_path\n",
        "                ], check=True, timeout=300)\n",
        "\n",
        "                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                    print(f\"Successfully copied video to MP4 container: {output_path}\")\n",
        "                    return output_path\n",
        "                else:\n",
        "                    print(\"Copy method failed to produce a valid output file\")\n",
        "                    # If all conversion methods fail, return the original file path\n",
        "                    return input_path\n",
        "            except Exception as e2:\n",
        "                print(f\"All conversion methods failed: {str(e2)}\")\n",
        "                # If all conversion methods fail, return the original file path\n",
        "                return input_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during conversion: {str(e)}\")\n",
        "        # Return the original file path if all else fails\n",
        "        return input_path\n",
        "\n",
        "def get_video_duration(file_path):\n",
        "    \"\"\"Get the duration of a video file in seconds using ffprobe\"\"\"\n",
        "    try:\n",
        "        # Use ffprobe to get the duration\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n",
        "             '-of', 'default=noprint_wrappers=1:nokey=1', file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "            check=True\n",
        "        )\n",
        "        duration = float(result.stdout.strip())\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {str(e)}\")\n",
        "        # Fall back to OpenCV\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(file_path)\n",
        "            if not cap.isOpened():\n",
        "                return 0\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            duration = frame_count / fps if fps > 0 else 0\n",
        "            cap.release()\n",
        "            return duration\n",
        "        except Exception as e2:\n",
        "            print(f\"Error getting duration with OpenCV: {str(e2)}\")\n",
        "            return 0\n",
        "\n",
        "def download_from_url(url):\n",
        "    \"\"\"Download video from URL and return local file path\"\"\"\n",
        "    # Create temp directory if it doesn't exist\n",
        "    temp_dir = \"temp_videos\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Generate a temporary filename without extension\n",
        "    timestamp = int(time.time())\n",
        "    file_base = os.path.join(temp_dir, f\"downloaded_video_{timestamp}\")\n",
        "    temp_file = f\"{file_base}.mp4\"\n",
        "\n",
        "    # Check if it's a YouTube URL or regular URL\n",
        "    if \"youtube.com\" in url or \"youtu.be\" in url:\n",
        "        try:\n",
        "            print(f\"Attempting to download YouTube video from: {url}\")\n",
        "\n",
        "            # Clean up YouTube URL - remove playlist parameters\n",
        "            if \"?list=\" in url:\n",
        "                url = url.split(\"?list=\")[0]\n",
        "            elif \"&list=\" in url:\n",
        "                url = url.split(\"&list=\")[0]\n",
        "\n",
        "            # Remove any additional query parameters\n",
        "            if \"?si=\" in url:\n",
        "                url = url.split(\"?si=\")[0]\n",
        "            elif \"&si=\" in url:\n",
        "                url = url.split(\"&si=\")[0]\n",
        "\n",
        "            print(f\"Using cleaned URL: {url}\")\n",
        "\n",
        "            # Skip pytube and use yt-dlp directly with format 22 (usually 720p MP4)\n",
        "            # This format tends to be more reliable with less need for conversion\n",
        "            print(\"Downloading with yt-dlp using format 22 (720p MP4)...\")\n",
        "            try:\n",
        "                # Ensure yt-dlp is updated\n",
        "                !pip install -q --upgrade yt-dlp\n",
        "\n",
        "                # Use format 22 which is typically 720p MP4 (most compatible)\n",
        "                !yt-dlp -f 22 -o \"{temp_file}\" \"{url}\"\n",
        "\n",
        "                if os.path.exists(temp_file) and os.path.getsize(temp_file) > 0:\n",
        "                    print(f\"Successfully downloaded video with format 22 to {temp_file}\")\n",
        "                    # This format should be H.264 compatible, but verify to be sure\n",
        "                    return ensure_h264_mp4(temp_file, temp_dir)\n",
        "                else:\n",
        "                    # Try method 2 with best format\n",
        "                    raise Exception(\"Format 22 download failed. Trying best format...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"First download method failed: {str(e)}. Trying best format...\")\n",
        "\n",
        "                # Method 2: Use 'best' format\n",
        "                try:\n",
        "                    print(\"Downloading with yt-dlp using 'best' format...\")\n",
        "                    !yt-dlp -f \"best\" -o \"{temp_file}\" \"{url}\"\n",
        "\n",
        "                    if os.path.exists(temp_file) and os.path.getsize(temp_file) > 0:\n",
        "                        print(f\"Successfully downloaded video with 'best' format to {temp_file}\")\n",
        "                        return ensure_h264_mp4(temp_file, temp_dir)\n",
        "                    else:\n",
        "                        # Try method 3 with direct download\n",
        "                        raise Exception(\"'Best' format download failed. Trying direct download...\")\n",
        "\n",
        "                except Exception as e2:\n",
        "                    print(f\"Second download method failed: {str(e2)}. Trying direct link...\")\n",
        "\n",
        "                    # Try using yt-dlp to get direct URL then download with urllib\n",
        "                    try:\n",
        "                        print(\"Getting direct video URL from YouTube...\")\n",
        "                        import json\n",
        "                        # Get the info as JSON and extract direct URL\n",
        "                        info_cmd = f\"yt-dlp -f 18 -j {url}\"\n",
        "                        result = !{info_cmd}\n",
        "                        if result:\n",
        "                            info = json.loads(result[0])\n",
        "                            direct_url = info.get('url')\n",
        "                            if direct_url:\n",
        "                                print(f\"Got direct URL, downloading with urllib...\")\n",
        "                                urllib.request.urlretrieve(direct_url, temp_file)\n",
        "                                if os.path.exists(temp_file) and os.path.getsize(temp_file) > 0:\n",
        "                                    print(f\"Successfully downloaded with direct URL to {temp_file}\")\n",
        "                                    return ensure_h264_mp4(temp_file, temp_dir)\n",
        "\n",
        "                        raise Exception(\"Could not get direct URL from YouTube\")\n",
        "                    except Exception as e3:\n",
        "                        print(f\"All download methods failed: {str(e3)}\")\n",
        "                        raise Exception(\"Could not download video from YouTube after multiple attempts\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"YouTube download failed: {str(e)}\")\n",
        "    else:\n",
        "        # Download regular URL\n",
        "        try:\n",
        "            print(f\"Downloading from direct URL: {url}\")\n",
        "            urllib.request.urlretrieve(url, temp_file)\n",
        "            print(f\"Successfully downloaded to {temp_file}\")\n",
        "\n",
        "            # Ensure it's in H.264 format\n",
        "            return ensure_h264_mp4(temp_file, temp_dir)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error downloading video: {str(e)}\")\n",
        "\n",
        "# Function to estimate depth for a single frame\n",
        "def estimate_depth(frame, model, transform, device):\n",
        "    \"\"\"Estimate depth for a single frame using MiDaS\"\"\"\n",
        "    # Preprocess image for MiDaS (using torch.hub transforms)\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    # Compute depth prediction\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_batch)\n",
        "        # Resize prediction to original frame size\n",
        "        prediction = torch.nn.functional.interpolate(\n",
        "            prediction.unsqueeze(1),\n",
        "            size=frame.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze()\n",
        "\n",
        "    depth = prediction.cpu().numpy()\n",
        "\n",
        "    # Normalize depth map to 0-1 range\n",
        "    depth_min = depth.min()\n",
        "    depth_max = depth.max()\n",
        "    if depth_max - depth_min > 0:\n",
        "        depth = (depth - depth_min) / (depth_max - depth_min)\n",
        "    else:\n",
        "        depth = np.zeros(depth.shape, dtype=depth.dtype)\n",
        "\n",
        "    return depth\n",
        "\n",
        "# Process batches of frames efficiently\n",
        "def process_batch(frames, model, transform, device):\n",
        "    \"\"\"Process a batch of frames to get depth maps\"\"\"\n",
        "    depth_maps = []\n",
        "\n",
        "    # Process each frame in the batch separately\n",
        "    # This is more compatible than trying to batch process\n",
        "    for frame in frames:\n",
        "        depth_map = estimate_depth(frame, model, transform, device)\n",
        "        depth_maps.append(depth_map)\n",
        "\n",
        "    return depth_maps"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stereo_conversion"
      },
      "source": [
        "## Stereoscopic Conversion\n",
        "\n",
        "Functions to create stereoscopic side-by-side views from original frames and depth maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stereo_functions"
      },
      "source": [
        "def create_depth_based_disparity(depth_map, depth_intensity, convergence, eye_separation):\n",
        "    \"\"\"Create disparity map from depth map using the control parameters\"\"\"\n",
        "    # Invert depth map since closer objects should have larger disparity\n",
        "    inverted_depth = 1.0 - depth_map\n",
        "\n",
        "    # Apply intensity control\n",
        "    disparity = inverted_depth * depth_intensity\n",
        "\n",
        "    # Apply eye separation and convergence adjustment\n",
        "    disparity = disparity * eye_separation / convergence\n",
        "\n",
        "    return disparity\n",
        "\n",
        "def generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation):\n",
        "    \"\"\"Generate left and right eye views for stereoscopic 3D\"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    # Create disparity map from depth map\n",
        "    disparity = create_depth_based_disparity(depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "    # Scale disparity to pixel displacement (max 5% of image width)\n",
        "    max_shift = int(w * 0.05)\n",
        "    disparity_scaled = disparity * max_shift\n",
        "\n",
        "    # Create empty images for left and right views\n",
        "    left_view = np.zeros_like(frame)\n",
        "    right_view = np.zeros_like(frame)\n",
        "\n",
        "    # For each row in the image\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            # Calculate shift for this pixel\n",
        "            shift = disparity_scaled[y, x]\n",
        "\n",
        "            # Calculate left and right positions\n",
        "            left_x = max(0, min(w-1, int(x - shift/2)))\n",
        "            right_x = max(0, min(w-1, int(x + shift/2)))\n",
        "\n",
        "            # Copy pixel values\n",
        "            left_view[y, left_x] = frame[y, x]\n",
        "            right_view[y, right_x] = frame[y, x]\n",
        "\n",
        "    # Fill holes using inpainting\n",
        "    # Create masks for unfilled areas\n",
        "    left_mask = np.all(left_view == 0, axis=2).astype(np.uint8) * 255\n",
        "    right_mask = np.all(right_view == 0, axis=2).astype(np.uint8) * 255\n",
        "\n",
        "    # Inpainting\n",
        "    if np.any(left_mask):\n",
        "        left_view = cv2.inpaint(left_view, left_mask, 3, cv2.INPAINT_TELEA)\n",
        "    if np.any(right_mask):\n",
        "        right_view = cv2.inpaint(right_view, right_mask, 3, cv2.INPAINT_TELEA)\n",
        "\n",
        "    return left_view, right_view\n",
        "\n",
        "def create_side_by_side(left_view, right_view):\n",
        "    \"\"\"Combine left and right views into a side-by-side 3D format with 16:9 overall aspect ratio\n",
        "    and 4:3 aspect ratio for each eye. Each eye view is embedded in a 16:9 frame with black bars.\"\"\"\n",
        "\n",
        "    # Fixed dimensions for the final 16:9 output\n",
        "    total_width = 1920   # Total width for 16:9 aspect ratio\n",
        "    total_height = 1080  # Total height for 16:9 aspect ratio\n",
        "\n",
        "    # Each eye gets half the width\n",
        "    eye_width = total_width // 2  # 960px per eye\n",
        "\n",
        "    # Determine content height for 4:3 aspect ratio within each eye view\n",
        "    content_height = int(eye_width * 3/4)  # 720px for 4:3 ratio at 960px width\n",
        "\n",
        "    # Resize views to exact 4:3 dimensions for each eye\n",
        "    left_resized = cv2.resize(left_view, (eye_width, content_height))\n",
        "    right_resized = cv2.resize(right_view, (eye_width, content_height))\n",
        "\n",
        "    # Create a black canvas with 16:9 aspect ratio\n",
        "    sbs_frame = np.zeros((total_height, total_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Calculate vertical offset to center content (black bars at top and bottom)\n",
        "    vertical_offset = (total_height - content_height) // 2\n",
        "\n",
        "    # Place the views side by side in the center of the frame with black bars\n",
        "    sbs_frame[vertical_offset:vertical_offset+content_height, 0:eye_width] = left_resized\n",
        "    sbs_frame[vertical_offset:vertical_offset+content_height, eye_width:total_width] = right_resized\n",
        "\n",
        "    return sbs_frame"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "processing_function"
      },
      "source": [
        "## Main Video Processing Function\n",
        "\n",
        "Implements the core processing pipeline that converts the 2D video to 3D SBS format with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_processing"
      },
      "source": [
        "def process_video_to_3d_sbs(input_path, output_path, depth_intensity, convergence, eye_separation,\n",
        "                           progress=None, use_segment=False, segment_start=0, segment_end=None):\n",
        "    \"\"\"Convert a 2D video to 3D SBS using MiDaS depth estimation with GPU optimization\"\"\"\n",
        "    try:\n",
        "        # Validate input video\n",
        "        valid, result = validate_video(input_path)\n",
        "        if not valid:\n",
        "            raise ValueError(result)\n",
        "\n",
        "        video_info = result\n",
        "\n",
        "        # Create temporary directory for intermediate files\n",
        "        temp_dir = \"temp_videos\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Base names for temp files\n",
        "        timestamp = int(time.time())\n",
        "        temp_base = os.path.join(temp_dir, f\"temp_{timestamp}\")\n",
        "        temp_video_path = f\"{temp_base}_video.mp4\"  # For video without audio\n",
        "        temp_audio_path = f\"{temp_base}_audio.aac\"  # For extracted audio\n",
        "\n",
        "        # Track whether we're processing a segment\n",
        "        is_segment = False\n",
        "        original_input = input_path\n",
        "\n",
        "        # Extract audio from the source video (original or segment)\n",
        "        has_audio = check_audio_stream(input_path)\n",
        "        if has_audio:\n",
        "            print(\"Detected audio stream in the video\")\n",
        "            if extract_audio(input_path, temp_audio_path):\n",
        "                print(f\"Audio extracted to {temp_audio_path}\")\n",
        "            else:\n",
        "                print(\"Could not extract audio. Output will have no sound.\")\n",
        "                has_audio = False\n",
        "        else:\n",
        "            print(\"No audio stream detected in the video\")\n",
        "\n",
        "        # If using a segment, extract it first\n",
        "        segment_path = None\n",
        "        if use_segment and segment_start is not None and segment_end is not None and segment_start < segment_end:\n",
        "            try:\n",
        "                # Create temporary segment file\n",
        "                temp_dir = \"temp_videos\"\n",
        "                os.makedirs(temp_dir, exist_ok=True)\n",
        "                segment_path = os.path.join(temp_dir, f\"segment_{int(time.time())}.mp4\")\n",
        "\n",
        "                # Extract the segment\n",
        "                segment_path = extract_video_segment(input_path, segment_path, segment_start, segment_end)\n",
        "\n",
        "                # Use the segment file for processing\n",
        "                input_path = segment_path\n",
        "\n",
        "                # Re-validate the segment\n",
        "                valid, result = validate_video(input_path)\n",
        "                if not valid:\n",
        "                    raise ValueError(f\"Segment validation failed: {result}\")\n",
        "\n",
        "                video_info = result\n",
        "                print(f\"Using video segment from {segment_start}s to {segment_end}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting segment: {str(e)}. Processing entire video instead.\")\n",
        "                # Continue with the original file if segment extraction fails\n",
        "\n",
        "        width, height = video_info[\"width\"], video_info[\"height\"]\n",
        "        fps = video_info[\"fps\"]\n",
        "        frame_count = int(video_info[\"frame_count\"])\n",
        "\n",
        "        # Clear GPU memory before starting\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Setup MiDaS model - note: only getting 3 return values now\n",
        "        model, transform, device = setup_midas()\n",
        "\n",
        "        # Determine optimal batch size based on available GPU memory and resolution\n",
        "        batch_size = 1  # Default\n",
        "        if torch.cuda.is_available():\n",
        "            # Calculate available memory\n",
        "            available_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "            current_mem = torch.cuda.memory_allocated()\n",
        "            free_mem = available_mem - current_mem\n",
        "\n",
        "            # Heuristic for batch size based on resolution\n",
        "            pixel_count = width * height\n",
        "            if pixel_count <= 640 * 480:  # SD video\n",
        "                batch_size = 8\n",
        "            elif pixel_count <= 1280 * 720:  # HD video\n",
        "                batch_size = 4\n",
        "            elif pixel_count <= 1920 * 1080:  # Full HD\n",
        "                batch_size = 2\n",
        "            else:  # 4K\n",
        "                batch_size = 1\n",
        "\n",
        "            print(f\"Using batch size: {batch_size} for {width}x{height} video\")\n",
        "            print(f\"Available GPU memory: {free_mem/1e9:.2f}GB\")\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "        # Create output video writer\n",
        "        target_width, target_height = 1920, 1080  # 16:9 overall aspect ratio\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # H.264 codec\n",
        "        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (target_width, target_height))\n",
        "\n",
        "        # Process frames\n",
        "        frame_index = 0\n",
        "        prev_depth_map = None\n",
        "\n",
        "        # Report initial memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
        "\n",
        "        # Process video in batches\n",
        "        while True:\n",
        "            # Read batch of frames\n",
        "            frames = []\n",
        "            for _ in range(batch_size):\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frames.append(frame)\n",
        "\n",
        "            if not frames:\n",
        "                break  # End of video\n",
        "\n",
        "            # Process batch of frames to get depth maps\n",
        "            depth_maps = process_batch(frames, model, transform, device)\n",
        "\n",
        "            # Process each frame with its depth map\n",
        "            for i in range(len(frames)):\n",
        "                frame = frames[i]\n",
        "                depth_map = depth_maps[i]\n",
        "\n",
        "                # Apply temporal smoothing\n",
        "                if prev_depth_map is not None:\n",
        "                    depth_map = 0.8 * depth_map + 0.2 * prev_depth_map\n",
        "                prev_depth_map = depth_map.copy()\n",
        "\n",
        "                # Generate stereo views\n",
        "                left_view, right_view = generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "                # Create side-by-side frame\n",
        "                sbs_frame = create_side_by_side(left_view, right_view)\n",
        "\n",
        "                # Write frame to output\n",
        "                out.write(sbs_frame)\n",
        "\n",
        "                # Update progress\n",
        "                frame_index += 1\n",
        "                if progress is not None:\n",
        "                    progress(min(1.0, frame_index / frame_count))\n",
        "\n",
        "                # Report GPU memory periodically\n",
        "                if frame_index % 10 == 0 and torch.cuda.is_available():\n",
        "                    memory_used_gb = torch.cuda.memory_allocated(0)/1e9\n",
        "                    total_mem_gb = torch.cuda.get_device_properties(0).total_memory/1e9\n",
        "                    usage_percent = (memory_used_gb / total_mem_gb) * 100\n",
        "                    print(f\"Frame {frame_index}/{frame_count} - GPU Memory: {memory_used_gb:.2f}GB / {total_mem_gb:.2f}GB ({usage_percent:.1f}%)\")\n",
        "\n",
        "            # Clean GPU memory every 50 frames\n",
        "            if frame_index % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        # Ensure 100% progress at the end\n",
        "        if progress is not None:\n",
        "            progress(1.0)\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        print(\"Processing complete!\")\n",
        "\n",
        "        # Now combine the processed video with the original audio\n",
        "        if has_audio:\n",
        "            print(\"Combining video with original audio...\")\n",
        "            if combine_video_audio(temp_video_path, temp_audio_path, output_path):\n",
        "                print(\"Successfully combined video with audio\")\n",
        "            else:\n",
        "                print(\"Audio combination failed. Using high quality encoding for video-only output...\")\n",
        "                # Fall back to just processing the video without audio\n",
        "                if torch.cuda.is_available():\n",
        "                    subprocess.run([\n",
        "                        'ffmpeg',\n",
        "                        '-i', temp_video_path,\n",
        "                        '-c:v', 'h264_nvenc',  # NVIDIA hardware encoding\n",
        "                        '-preset', 'p2',       # Medium quality/speed\n",
        "                        '-b:v', '8M',          # Bitrate\n",
        "                        '-y',                  # Overwrite output if exists\n",
        "                        output_path\n",
        "                    ], check=True, timeout=600)\n",
        "                else:\n",
        "                    subprocess.run([\n",
        "                        'ffmpeg',\n",
        "                        '-i', temp_video_path,\n",
        "                        '-c:v', 'libx264',     # CPU encoding\n",
        "                        '-preset', 'medium',   # Medium quality/speed\n",
        "                        '-crf', '23',          # Quality level\n",
        "                        '-y',                  # Overwrite output if exists\n",
        "                        output_path\n",
        "                    ], check=True, timeout=600)\n",
        "        else:\n",
        "            # No audio to add, just convert the video\n",
        "            print(\"No audio to add. Finalizing video with high quality encoding...\")\n",
        "            if torch.cuda.is_available():\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', temp_video_path,\n",
        "                    '-c:v', 'h264_nvenc',  # NVIDIA hardware encoding\n",
        "                    '-preset', 'p2',       # Medium quality/speed\n",
        "                    '-b:v', '8M',          # Bitrate\n",
        "                    '-y',                  # Overwrite output if exists\n",
        "                    output_path\n",
        "                ], check=True, timeout=600)\n",
        "            else:\n",
        "                subprocess.run([\n",
        "                    'ffmpeg',\n",
        "                    '-i', temp_video_path,\n",
        "                    '-c:v', 'libx264',     # CPU encoding\n",
        "                    '-preset', 'medium',   # Medium quality/speed\n",
        "                    '-crf', '23',          # Quality level\n",
        "                    '-y',                  # Overwrite output if exists\n",
        "                    output_path\n",
        "                ], check=True, timeout=600)\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Clean up segment file if we created one\n",
        "        if segment_path and os.path.exists(segment_path) and segment_path != input_path:\n",
        "            try:\n",
        "                os.remove(segment_path)\n",
        "                print(f\"Cleaned up temporary segment file: {segment_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not remove temporary segment file: {str(e)}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in process_video_to_3d_sbs: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_preview_frame(input_path, depth_intensity, convergence, eye_separation, frame_position=0.5):\n",
        "    \"\"\"Generate a preview frame for the given parameters\"\"\"\n",
        "    try:\n",
        "        # Validate input video\n",
        "        valid, result = validate_video(input_path)\n",
        "        if not valid:\n",
        "            raise ValueError(result)\n",
        "\n",
        "        # Setup MiDaS model\n",
        "        model, transform, device = setup_midas()\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "        # Get frame count and set position\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        target_frame = int(frame_count * frame_position)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n",
        "\n",
        "        # Read frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            cap.release()\n",
        "            raise ValueError(\"Could not read frame from video\")\n",
        "\n",
        "        # Estimate depth\n",
        "        depth_map = estimate_depth(frame, model, transform, device)\n",
        "\n",
        "        # Generate stereo views\n",
        "        left_view, right_view = generate_stereo_views(frame, depth_map, depth_intensity, convergence, eye_separation)\n",
        "\n",
        "        # Create side-by-side frame\n",
        "        sbs_frame_full = create_side_by_side(left_view, right_view)\n",
        "\n",
        "        # Create preview (smaller version)\n",
        "        preview_height = 360\n",
        "        preview_width = int(1920 * (preview_height / 1080))\n",
        "        sbs_frame = cv2.resize(sbs_frame_full, (preview_width, preview_height))\n",
        "\n",
        "        # Create comparison view with original frame\n",
        "        h, w = frame.shape[:2]\n",
        "        original_resized = cv2.resize(frame, (int(w * preview_height / h), preview_height))\n",
        "\n",
        "        # Create final preview\n",
        "        preview_width_total = original_resized.shape[1] + sbs_frame.shape[1] + 10\n",
        "        preview = np.zeros((preview_height, preview_width_total, 3), dtype=np.uint8)\n",
        "\n",
        "        # Add original frame\n",
        "        preview[:, :original_resized.shape[1]] = original_resized\n",
        "        # Add SBS frame\n",
        "        preview[:, original_resized.shape[1]+10:] = sbs_frame\n",
        "\n",
        "        # Add labels\n",
        "        cv2.putText(preview, \"Original\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(preview, \"3D SBS (16:9)\", (original_resized.shape[1]+20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "\n",
        "        # Convert to RGB for display\n",
        "        preview_rgb = cv2.cvtColor(preview, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        return preview_rgb\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_preview_frame: {str(e)}\")\n",
        "        raise"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gradio_interface"
      },
      "source": [
        "## Gradio Interface\n",
        "\n",
        "Create an intuitive user interface for the 2D to 3D conversion with parameter controls and real-time preview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio_app"
      },
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create and launch the Gradio interface for 3D SBS conversion with audio support\"\"\"\n",
        "    # Global variables for state management\n",
        "    input_video_path = None\n",
        "    output_video_path = None\n",
        "    video_duration = 0  # Store video duration for segment selection\n",
        "\n",
        "    def save_uploaded_file(file_obj):\n",
        "        \"\"\"Helper function to save an uploaded file to disk\"\"\"\n",
        "        temp_dir = \"temp_videos\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Generate a filename with timestamp to avoid conflicts\n",
        "        timestamp = int(time.time())\n",
        "        file_name = f\"uploaded_video_{timestamp}.mp4\"\n",
        "        file_path = os.path.join(temp_dir, file_name)\n",
        "\n",
        "        print(f\"Saving uploaded file to {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Handle different file object types based on Gradio version\n",
        "            if isinstance(file_obj, str):\n",
        "                # It's a file path string, just copy the file\n",
        "                shutil.copy(file_obj, file_path)\n",
        "            elif hasattr(file_obj, 'name') and os.path.exists(file_obj.name):\n",
        "                # It's an object with a name attribute that points to a real file\n",
        "                shutil.copy(file_obj.name, file_path)\n",
        "            else:\n",
        "                # Try multiple approaches based on different versions of Gradio\n",
        "                if hasattr(file_obj, 'read') and callable(file_obj.read):\n",
        "                    # It's a file-like object, read and write it\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        f.write(file_obj.read())\n",
        "                elif hasattr(file_obj, '_path') and os.path.exists(file_obj._path):\n",
        "                    # Some versions of Gradio use a _path attribute\n",
        "                    shutil.copy(file_obj._path, file_path)\n",
        "                else:\n",
        "                    # Fall back to trying to directly access file object (may not work in all cases)\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        if isinstance(file_obj, bytes):\n",
        "                            f.write(file_obj)\n",
        "                        else:\n",
        "                            f.write(str(file_obj).encode('utf-8'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving file: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "        # Ensure the video is in H.264 format\n",
        "        return ensure_h264_mp4(file_path, temp_dir)\n",
        "\n",
        "    def upload_video(video_file):\n",
        "        \"\"\"Handle video upload\"\"\"\n",
        "        nonlocal input_video_path, video_duration\n",
        "\n",
        "        if video_file is None:\n",
        "            return None, \"Please upload a video file\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "        try:\n",
        "            # Save the uploaded file to disk and ensure H.264 encoding\n",
        "            input_video_path = save_uploaded_file(video_file)\n",
        "\n",
        "            # Validate video\n",
        "            valid, result = validate_video(input_video_path)\n",
        "            if not valid:\n",
        "                return None, result, gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "            # Get video duration for segment selection\n",
        "            video_duration = result.get(\"duration_sec\", 0)\n",
        "            if video_duration <= 0:\n",
        "                video_duration = get_video_duration(input_video_path)\n",
        "\n",
        "            # Update segment sliders\n",
        "            start_slider = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "            end_slider = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Generate a preview frame\n",
        "            preview = generate_preview_frame(input_video_path, 0.5, 5.0, 2.5)\n",
        "\n",
        "            # Enable segment checkbox only if video is longer than 30 seconds\n",
        "            enable_segment = video_duration > 30\n",
        "\n",
        "            return preview, f\"Video loaded successfully: {result['width']}x{result['height']}, {result['fps']:.2f} FPS, {result['frame_count']} frames, {result['size_mb']:.2f}MB, Duration: {video_duration:.2f}s\", start_slider, end_slider, enable_segment\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing video upload: {str(e)}\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "    def download_from_url_handler(url):\n",
        "        \"\"\"Handle video URL input\"\"\"\n",
        "        nonlocal input_video_path, video_duration\n",
        "\n",
        "        if not url or url.strip() == \"\":\n",
        "            return None, \"Please enter a valid URL\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "        try:\n",
        "            # Download video and convert to H.264 if needed\n",
        "            input_video_path = download_from_url(url)\n",
        "\n",
        "            # Validate video\n",
        "            valid, result = validate_video(input_video_path)\n",
        "            if not valid:\n",
        "                return None, result, gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "            # Get video duration for segment selection\n",
        "            video_duration = result.get(\"duration_sec\", 0)\n",
        "            if video_duration <= 0:\n",
        "                video_duration = get_video_duration(input_video_path)\n",
        "\n",
        "            # Update segment sliders\n",
        "            start_slider = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "            end_slider = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Generate a preview frame\n",
        "            preview = generate_preview_frame(input_video_path, 0.5, 5.0, 2.5)\n",
        "\n",
        "            # Enable segment checkbox only if video is longer than 30 seconds\n",
        "            enable_segment = video_duration > 30\n",
        "\n",
        "            return preview, f\"Video downloaded and converted successfully: {result['width']}x{result['height']}, {result['fps']:.2f} FPS, {result['frame_count']} frames, {result['size_mb']:.2f}MB, Duration: {video_duration:.2f}s\", start_slider, end_slider, enable_segment\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error downloading video: {str(e)}\", gr.Slider(minimum=0, maximum=1), gr.Slider(minimum=0, maximum=1), False\n",
        "\n",
        "    def update_preview(depth_intensity, convergence, eye_separation):\n",
        "        \"\"\"Update preview based on parameter changes\"\"\"\n",
        "        nonlocal input_video_path\n",
        "\n",
        "        if input_video_path is None or not os.path.exists(input_video_path):\n",
        "            return None, \"No video loaded\"\n",
        "\n",
        "        try:\n",
        "            # Generate new preview with current parameters\n",
        "            preview = generate_preview_frame(input_video_path, depth_intensity, convergence, eye_separation)\n",
        "            return preview, \"Preview updated with new parameters\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Error updating preview: {str(e)}\"\n",
        "\n",
        "    def update_end_time(start_time):\n",
        "        \"\"\"Update the end time slider to ensure it's always greater than start time\"\"\"\n",
        "        return gr.Slider(minimum=start_time + 0.1, maximum=video_duration, value=max(start_time + 0.1, video_duration))\n",
        "\n",
        "    def sync_segment_values(use_segment, segment_start, segment_end):\n",
        "        \"\"\"Synchronize segment values between tabs\"\"\"\n",
        "        # For Gradio compatibility, return a tuple of values instead of a dictionary\n",
        "        return use_segment, segment_start, segment_end\n",
        "\n",
        "    def process_video(depth_intensity, convergence, eye_separation, use_segment, segment_start, segment_end, progress=gr.Progress()):\n",
        "        \"\"\"Process the video with the given parameters\"\"\"\n",
        "        nonlocal input_video_path, output_video_path, video_duration\n",
        "\n",
        "        if input_video_path is None or not os.path.exists(input_video_path):\n",
        "            return None, \"No video loaded\"\n",
        "\n",
        "        try:\n",
        "            # Create output directory\n",
        "            output_dir = \"output_videos\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Generate output filename\n",
        "            base_name = os.path.basename(input_video_path)\n",
        "            name, ext = os.path.splitext(base_name)\n",
        "\n",
        "            # Add segment info to output filename if using segment\n",
        "            if use_segment and segment_start is not None and segment_end is not None and segment_start < segment_end:\n",
        "                output_video_path = os.path.join(output_dir, f\"{name}_3D_SBS_{int(segment_start)}-{int(segment_end)}s.mp4\")\n",
        "            else:\n",
        "                output_video_path = os.path.join(output_dir, f\"{name}_3D_SBS.mp4\")\n",
        "\n",
        "            # Process video\n",
        "            process_video_to_3d_sbs(\n",
        "                input_path=input_video_path,\n",
        "                output_path=output_video_path,\n",
        "                depth_intensity=depth_intensity,\n",
        "                convergence=convergence,\n",
        "                eye_separation=eye_separation,\n",
        "                progress=progress,\n",
        "                use_segment=use_segment,\n",
        "                segment_start=segment_start if use_segment else None,\n",
        "                segment_end=segment_end if use_segment else None\n",
        "            )\n",
        "\n",
        "            segment_info = f\" (segment {segment_start:.1f}s-{segment_end:.1f}s)\" if use_segment else \"\"\n",
        "            return output_video_path, f\"Video processed successfully{segment_info}. Saved to {output_video_path} with 16:9 aspect ratio (1920x1080) as requested.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Error processing video: {str(e)}\"\n",
        "\n",
        "    # Create the Gradio interface\n",
        "    with gr.Blocks(title=\"2D to 3D SBS Video Converter (GPU Optimized)\") as app:\n",
        "        gr.Markdown(\"# 2D to 3D Side-by-Side Video Converter (GPU Optimized)\")\n",
        "        gr.Markdown(\"Convert standard 2D videos to stereoscopic 3D SBS format for VR viewing. Output has a 16:9 aspect ratio (1920x1080) with both eye views side by side. **Preserves original audio track** in the output video.\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_info = f\"Using GPU: {torch.cuda.get_device_name(0)} with {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB memory\"\n",
        "            gr.Markdown(f\"**{gpu_info}**\")\n",
        "        else:\n",
        "            gr.Markdown(\"**Running in CPU mode. Processing will be slower without GPU acceleration.**\")\n",
        "\n",
        "        with gr.Tab(\"Upload Video\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Video upload widget\n",
        "                    upload_input = gr.File(\n",
        "                        label=\"Upload Video File (max 500MB)\",\n",
        "                        file_types=[\"video\"],\n",
        "                        file_count=\"single\"\n",
        "                    )\n",
        "                    upload_button = gr.Button(\"Upload and Preview\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status\n",
        "                    preview = gr.Image(label=\"Preview\")\n",
        "                    status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Segment selection (initially hidden/disabled)\n",
        "                    use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False, interactive=False)\n",
        "                    segment_start = gr.Slider(minimum=0, maximum=1, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    segment_end = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Connect upload button\n",
        "            upload_button.click(\n",
        "                upload_video,\n",
        "                inputs=[upload_input],\n",
        "                outputs=[preview, status, segment_start, segment_end, use_segment]\n",
        "            )\n",
        "\n",
        "            # Update end time slider when start time changes to maintain valid range\n",
        "            segment_start.change(update_end_time, inputs=[segment_start], outputs=[segment_end])\n",
        "\n",
        "        with gr.Tab(\"Video URL\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # URL input widget\n",
        "                    url_input = gr.Textbox(label=\"Video URL (YouTube or direct link)\")\n",
        "                    url_button = gr.Button(\"Download and Preview\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status (shared with upload tab)\n",
        "                    url_preview = gr.Image(label=\"Preview\")\n",
        "                    url_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Segment selection (initially hidden/disabled)\n",
        "                    url_use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False, interactive=False)\n",
        "                    url_segment_start = gr.Slider(minimum=0, maximum=1, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    url_segment_end = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "            # Connect URL button\n",
        "            url_button.click(\n",
        "                download_from_url_handler,\n",
        "                inputs=[url_input],\n",
        "                outputs=[url_preview, url_status, url_segment_start, url_segment_end, url_use_segment]\n",
        "            )\n",
        "\n",
        "            # Update end time slider when start time changes to maintain valid range\n",
        "            url_segment_start.change(update_end_time, inputs=[url_segment_start], outputs=[url_segment_end])\n",
        "\n",
        "        with gr.Tab(\"Convert to 3D\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Depth control parameters\n",
        "                    depth_intensity = gr.Slider(\n",
        "                        minimum=0.0, maximum=1.0, value=0.5, step=0.01,\n",
        "                        label=\"Depth Intensity\",\n",
        "                        info=\"Controls the strength of the 3D effect (0.0-1.0)\"\n",
        "                    )\n",
        "\n",
        "                    convergence = gr.Slider(\n",
        "                        minimum=1.0, maximum=10.0, value=5.0, step=0.1,\n",
        "                        label=\"Convergence Distance\",\n",
        "                        info=\"Adjusts the perceived distance of objects (1.0-10.0)\"\n",
        "                    )\n",
        "\n",
        "                    eye_separation = gr.Slider(\n",
        "                        minimum=0.1, maximum=5.0, value=2.5, step=0.1,\n",
        "                        label=\"Eye Separation\",\n",
        "                        info=\"Controls the distance between virtual cameras (0.1-5.0)\"\n",
        "                    )\n",
        "\n",
        "                    # Segment selection (duplicated for this tab for better UX)\n",
        "                    conv_use_segment = gr.Checkbox(label=\"Process a specific segment of the video\", value=False)\n",
        "                    conv_segment_start = gr.Slider(minimum=0, maximum=video_duration, value=0, step=0.1, label=\"Start Time (seconds)\")\n",
        "                    conv_segment_end = gr.Slider(minimum=0, maximum=video_duration, value=video_duration, step=0.1, label=\"End Time (seconds)\")\n",
        "\n",
        "                    # Update end time slider when start time changes\n",
        "                    conv_segment_start.change(update_end_time, inputs=[conv_segment_start], outputs=[conv_segment_end])\n",
        "\n",
        "                    # Update preview button\n",
        "                    update_button = gr.Button(\"Update Preview\")\n",
        "\n",
        "                    # Process button\n",
        "                    process_button = gr.Button(\"Process Video\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    # Preview and status (shared)\n",
        "                    convert_preview = gr.Image(label=\"Preview\")\n",
        "                    convert_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                    # Output video\n",
        "                    output_video = gr.Video(label=\"Converted 3D SBS Video (16:9 aspect ratio with audio)\")\n",
        "\n",
        "            # Connect update preview button\n",
        "            update_button.click(\n",
        "                update_preview,\n",
        "                inputs=[depth_intensity, convergence, eye_separation],\n",
        "                outputs=[convert_preview, convert_status]\n",
        "            )\n",
        "\n",
        "            # Connect process button\n",
        "            process_button.click(\n",
        "                process_video,\n",
        "                inputs=[depth_intensity, convergence, eye_separation, conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[output_video, convert_status]\n",
        "            )\n",
        "\n",
        "            # Synchronize segment values between tabs\n",
        "            # Connect the segment controls to the sync function\n",
        "            use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[use_segment, segment_start, segment_end],\n",
        "                outputs=[conv_use_segment, conv_segment_start, conv_segment_end]\n",
        "            )\n",
        "\n",
        "            url_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[url_use_segment, url_segment_start, url_segment_end],\n",
        "                outputs=[conv_use_segment, conv_segment_start, conv_segment_end]\n",
        "            )\n",
        "\n",
        "            # Sync back from Convert tab to others\n",
        "            conv_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[use_segment, segment_start, segment_end]\n",
        "            )\n",
        "\n",
        "            conv_use_segment.change(\n",
        "                sync_segment_values,\n",
        "                inputs=[conv_use_segment, conv_segment_start, conv_segment_end],\n",
        "                outputs=[url_use_segment, url_segment_start, url_segment_end]\n",
        "            )\n",
        "\n",
        "        # Help tab\n",
        "        with gr.Tab(\"Help\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## How to Use This Tool\n",
        "\n",
        "            1. Upload a video file or provide a URL to a video (supports YouTube).\n",
        "            2. For longer videos, you can choose to process only a specific segment to save time and memory:\n",
        "               - Check the \"Process a specific segment\" box\n",
        "               - Set the start and end times in seconds\n",
        "            3. Adjust the depth parameters to control the 3D effect:\n",
        "               - **Depth Intensity**: Controls the strength of the 3D effect. Higher values create more pronounced depth.\n",
        "               - **Convergence Distance**: Adjusts where objects appear to be in relation to the screen plane.\n",
        "               - **Eye Separation**: Controls the virtual camera separation. Higher values create more extreme 3D effects.\n",
        "            4. Click \"Update Preview\" to see how your settings affect the 3D output.\n",
        "            5. Click \"Process Video\" to convert the entire video (or selected segment) to 3D SBS format.\n",
        "            6. Download the converted video for viewing in a VR headset or 3D display.\n",
        "\n",
        "            ## Video Segmentation\n",
        "\n",
        "            The video segment feature allows you to process only a portion of a longer video. This is useful for:\n",
        "            - Testing different 3D settings on a small clip before processing the entire video\n",
        "            - Processing very long videos in manageable chunks to avoid memory issues or timeouts\n",
        "            - Creating highlights in 3D from specific parts of a longer video\n",
        "\n",
        "            ## Output Format\n",
        "\n",
        "            - The final video will have a 16:9 aspect ratio (1920x1080)\n",
        "            - Each eye view is positioned side by side with appropriate proportions\n",
        "            - Black bars are added as needed to maintain the proper 16:9 aspect ratio\n",
        "            - H.264 encoded MP4 format for maximum compatibility\n",
        "            - Maintains the original video's frame rate\n",
        "\n",
        "            ## Supported Formats\n",
        "\n",
        "            - Input: MP4, AVI, MOV, WebM, MKV (up to 4K resolution, max 500MB)\n",
        "            - Output: H.264 encoded MP4 in Side-by-Side format (1920x1080)\n",
        "\n",
        "            ## Viewing the 3D Video\n",
        "\n",
        "            The output video is in Side-by-Side (SBS) format, which can be viewed in:\n",
        "            - VR headsets using video players that support SBS format\n",
        "            - 3D TVs with SBS viewing mode\n",
        "            - Special 3D viewers like Google Cardboard with SBS-compatible apps\n",
        "\n",
        "            ## GPU Optimization\n",
        "\n",
        "            This version of the converter is optimized to take advantage of NVIDIA GPUs for faster processing:\n",
        "\n",
        "            - Batch processing of multiple frames at once to maximize GPU utilization\n",
        "            - GPU-accelerated depth map generation\n",
        "            - Optimized memory management to handle larger videos\n",
        "            - Hardware-accelerated video encoding when available\n",
        "\n",
        "            ## Troubleshooting\n",
        "\n",
        "            - If processing fails, try using a smaller segment of the video.\n",
        "            - For best results, use videos with good lighting and clear objects.\n",
        "            - If the 3D effect is too strong or causes discomfort, lower the Depth Intensity and Eye Separation values.\n",
        "            - If you experience issues with YouTube downloads, try using a direct video URL instead.\n",
        "            - If you experience any issues, check the status messages for error details.\n",
        "            \"\"\")\n",
        "\n",
        "    # Launch the app\n",
        "    app.launch(debug=True, share=True)\n",
        "\n",
        "# Initialize and launch the Gradio application\n",
        "create_gradio_interface()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## Troubleshooting and Tips\n",
        "\n",
        "If you experience issues with the application, here are some tips and solutions:\n",
        "\n",
        "1. **GPU Memory Errors**: If you encounter CUDA out of memory errors:\n",
        "   - Use the segment feature to process smaller portions of the video\n",
        "   - Restart the runtime to clear memory\n",
        "   - Process a shorter or lower resolution video\n",
        "\n",
        "2. **Loading Time**: The MiDaS model takes time to download and load initially. Be patient during first use.\n",
        "\n",
        "3. **Quality Issues**: The quality of the 3D effect depends on the input video quality and the accuracy of the depth map. Videos with clear objects and good lighting work best.\n",
        "\n",
        "4. **Processing Speed**: Even with GPU acceleration, depth estimation is computationally intensive. Processing time depends on video length, resolution, and available GPU resources.\n",
        "\n",
        "5. **View Distance**: If objects appear too close or too far in the 3D output, adjust the Convergence Distance parameter.\n",
        "\n",
        "6. **Eye Strain**: If the 3D effect causes discomfort, reduce the Depth Intensity and Eye Separation values for a more comfortable viewing experience.\n",
        "\n",
        "7. **YouTube Downloads**: If YouTube downloads fail, try using a different web browser to copy a direct video URL.\n",
        "\n",
        "8. **Runtime Disconnections**: For long videos, Colab might disconnect. Use the segment feature to process the video in chunks.\n",
        "\n",
        "9. **Video Conversion**: If video conversion seems stuck, try restarting the notebook and using a smaller video segment.\n",
        "\n",
        "10. **Maximizing GPU Usage**: This version attempts to use the full capacity of your GPU. You can monitor GPU usage in Colab using the command: `!nvidia-smi` in a new cell.\n",
        "\n",
        "11. **Processing Segments**: For videos longer than a few minutes, it's recommended to process them in 1-3 minute segments to avoid Colab timeouts and memory issues.\n",
        "\n",
        "12. **Best Results**: For optimal 3D conversion, use videos with:\n",
        "   - Clear foreground and background separation\n",
        "   - Good lighting conditions\n",
        "   - Minimal fast camera movement\n",
        "   - Higher resolution (1080p or above)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2D_to_3D_SBS_Converter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}